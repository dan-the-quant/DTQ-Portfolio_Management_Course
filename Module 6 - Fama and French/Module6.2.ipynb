{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb95aa40-adff-4669-bf50-5ec2373212db",
   "metadata": {},
   "source": [
    "# Fama and French Factor Model #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a23296-71d4-4783-9519-5c8a844d3f38",
   "metadata": {},
   "source": [
    "### Import Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdbc010-16d6-44dd-8580-2cb3e8cc947a",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "# Data Management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Manipulate Files\n",
    "import os\n",
    "\n",
    "# Pretty Notation\n",
    "from IPython.display import display, Math"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ca48f6-056c-4503-b64e-80c7fc827e8f",
   "metadata": {},
   "source": [
    "# Get the important data for the Risk Free Rate\n",
    "\n",
    "rfr = pd.read_csv(r\"..\\additional_data\\rfr.csv\")\n",
    "rfr = rfr.set_index('Date')\n",
    "rfr.index = pd.to_datetime(rfr.index, dayfirst=True)\n",
    "\n",
    "# Get the important data for the S&P500\n",
    "\n",
    "sp500 = pd.read_csv(r\"..\\additional_data\\sp500.csv\")\n",
    "sp500 = sp500.set_index('Date')\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Get the data for the Stocks' Betas\n",
    "\n",
    "betas_df = pd.read_csv(r\"..\\additional_data\\betas.csv\")\n",
    "betas_df = betas_df.set_index('Date')\n",
    "betas_df.index = pd.to_datetime(betas_df.index)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3eda35-9399-42d1-a3be-4e89f448b910",
   "metadata": {},
   "source": [
    "# Create the Weights function\n",
    "def wexp(N, half_life):\n",
    "    c = np.log(0.5)/half_life\n",
    "    n = np.array(range(N))\n",
    "    w = np.exp(c*n)\n",
    "    return np.flip(w/np.sum(w))\n",
    "\n",
    "# Create the CAPM \n",
    "def CAPM(\n",
    "    stock_prices: pd.Series, \n",
    "    benchmark_prices: pd.Series = sp500['sp_500'], \n",
    "    risk_free_rate: pd.Series = rfr['risk_free_rate'], \n",
    "    window: int = 252,\n",
    "    WLS: bool = True,\n",
    "):\n",
    "\n",
    "    # Align time series to the same date range\n",
    "    common_index = stock_prices.index.intersection(benchmark_prices.index).intersection(risk_free_rate.index)\n",
    "    stock_prices = stock_prices.loc[common_index]\n",
    "    benchmark_prices = benchmark_prices.loc[common_index]\n",
    "    risk_free_rate = risk_free_rate.loc[common_index]\n",
    "    \n",
    "    # Compute daily returns\n",
    "    stock_returns = stock_prices.pct_change(1)\n",
    "    benchmark_returns = benchmark_prices.pct_change(1)\n",
    "    risk_free_daily = (((1 + (risk_free_rate.div(100)))**(1/360)) - 1)  # Convert annual rate to daily\n",
    "    \n",
    "    # Excess returns\n",
    "    excess_stock = stock_returns - risk_free_daily\n",
    "    excess_benchmark = benchmark_returns - risk_free_daily\n",
    "\n",
    "    alphas, betas = [], []\n",
    "\n",
    "    # Create weights with exponential decay\n",
    "    weights = window * wexp(window, window/2)\n",
    "    \n",
    "    for t in range(window, len(stock_returns)):\n",
    "        X = excess_benchmark.iloc[t-window:t]\n",
    "        y = excess_stock.iloc[t-window:t]\n",
    "        \n",
    "        if X.isnull().any() or y.isnull().any():\n",
    "            continue\n",
    "\n",
    "        if WLS:\n",
    "            \n",
    "            # Fit WLS regression\n",
    "            model = sm.WLS(y, sm.add_constant(X), weights=weights, missing='drop').fit()\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Fit OLS regression\n",
    "            model = sm.OLS(y, sm.add_constant(X), missing='drop').fit()\n",
    "\n",
    "        # Avoid KeyError by checking if params exist\n",
    "        params = model.params\n",
    "        \n",
    "        alphas.append(params.iloc[0])\n",
    "        betas.append(params.iloc[1])\n",
    "            \n",
    "    parameters = pd.DataFrame({\n",
    "        'alpha': alphas,\n",
    "        'beta': betas,\n",
    "    }, index=stock_returns.index[window+1:])\n",
    "    \n",
    "    return parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "194144db-cd57-48af-a995-23e053ff15fd",
   "metadata": {},
   "source": [
    "# Folder Path\n",
    "folder_path = r\"..\\stocks\"\n",
    "\n",
    "# Dictionary to store the DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# List all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Full path to the file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Read the Excel file\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.set_index(\"Date\")\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        df = df[['Adjusted_close', 'Market_cap_calculado', 'Price_to_Book_inverse']]\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            'Adjusted_close':'adj_close',\n",
    "            'Market_cap_calculado':'market_cap',\n",
    "            'Price_to_Book_inverse':'book_to_price',\n",
    "        })\n",
    "\n",
    "        # Fill nans\n",
    "        df['adj_close'] = df['adj_close'].interpolate(method='linear')\n",
    "        df['market_cap'] = df['market_cap'].interpolate(method='linear')\n",
    "        df['book_to_price'] = df['book_to_price'].interpolate(method='linear')\n",
    "\n",
    "        df = df.loc['2015-01-01':]\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        if len(df) >= 2000:\n",
    "            # File name without extension\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Guardar en el diccionario\n",
    "            dataframes[file_name] = df\n",
    "            print(f\"File loaded: {file_name} ({len(df)} rows)\")\n",
    "        else:\n",
    "            print(f\"File skipped (less than 2000 rows after cleaning): {file}\")\n",
    "\n",
    "print(f\"\\nTotal files loaded: {len(dataframes)}\")\n",
    "print(\"Files loaded:\", list(dataframes.keys()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9628cf0d-0761-44af-9ed8-cb706879bb6e",
   "metadata": {},
   "source": [
    "# Create a whole new dataframe that contains all the stocks betas\n",
    "\n",
    "rets_series = []\n",
    "\n",
    "for stock, df in dataframes.items():\n",
    "    series = df['adj_close'].pct_change(1).rename(stock)  \n",
    "    series = series.iloc[1:]\n",
    "    rets_series.append(series)\n",
    "\n",
    "# Concat\n",
    "returns_df = pd.concat(rets_series, axis=1)\n",
    "returns_df = returns_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "# Drop nans\n",
    "returns_df.dropna(inplace = True)\n",
    "\n",
    "returns_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f158a5-a29a-49db-be3e-11b11bbafcc9",
   "metadata": {},
   "source": [
    "# Create a whole new dataframe that contains all the stocks betas\n",
    "\n",
    "mktcap_series = []\n",
    "\n",
    "for stock, df in dataframes.items():\n",
    "    series = df['market_cap'].rename(stock)  \n",
    "    series = series.iloc[1:]\n",
    "    mktcap_series.append(series)\n",
    "\n",
    "# Concat\n",
    "mktcap_df = pd.concat(mktcap_series, axis=1)\n",
    "mktcap_df = mktcap_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "# Drop nans\n",
    "mktcap_df.dropna(inplace = True)\n",
    "\n",
    "# Apply Logs and EMA (maybe)\n",
    "mktcap_df = np.log(mktcap_df)\n",
    "#mktcap_df = mktcap_df.ewm(span=21, adjust = False).mean()\n",
    "mktcap_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72e1f23-526e-45be-af18-b0177bb34c73",
   "metadata": {},
   "source": [
    "# Create a whole new dataframe that contains all the stocks betas\n",
    "\n",
    "btp_series = []\n",
    "\n",
    "for stock, df in dataframes.items():\n",
    "    series = df['book_to_price'].rename(stock)  \n",
    "    series = series.iloc[1:]\n",
    "    btp_series.append(series)\n",
    "\n",
    "# Concat\n",
    "btp_df = pd.concat(btp_series, axis=1)\n",
    "btp_df = btp_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "btp_df.dropna(inplace = True)\n",
    "#btp_df = btp_df.ewm(span=21, adjust = False).mean()\n",
    "\n",
    "btp_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b616c9a9-8452-489b-a3db-c68409b199f0",
   "metadata": {},
   "source": [
    "### Create the Fama & French Portfolios ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cddee2e5-e58e-47f3-afcc-5f25438583d1",
   "metadata": {},
   "source": [
    "# Define the Decomposition Function\n",
    "def fama_and_french_decomposition(\n",
    "    target_df, \n",
    "    mktcap_df, \n",
    "    value_df\n",
    "):\n",
    "    # Common Indexes\n",
    "    common_index = target_df.index.intersection(value_df.index).intersection(mktcap_df.index)\n",
    "    \n",
    "    # Reindex\n",
    "    target_df = target_df.loc[common_index]\n",
    "    mktcap_df = mktcap_df.loc[common_index]\n",
    "    value_df = value_df.loc[common_index]\n",
    "\n",
    "    # Initialize lists to store portfolio returns\n",
    "    small_low_list, small_neutral_list, small_high_list = [], [], []\n",
    "    big_low_list, big_neutral_list, big_high_list = [], [], []\n",
    "    \n",
    "    # Get unique quarters\n",
    "    quarters = sorted(set([date.to_period('Q') for date in common_index]))\n",
    "    \n",
    "    # Dictionary to store quarterly classifications and weights\n",
    "    quarterly_classifications = {}\n",
    "\n",
    "    for quarter in quarters:\n",
    "        # Select only the last available date of the quarter\n",
    "        quarter_dates = [date for date in common_index if date.to_period('Q') == quarter]\n",
    "        rebalance_date = quarter_dates[-1]  # Last day of the quarter\n",
    "        \n",
    "        # Size factor for rebalance date\n",
    "        size_factor_df = pd.DataFrame([mktcap_df.loc[rebalance_date]], index=['mkt_cap']).T.dropna()\n",
    "        \n",
    "        # Value factor (P/B ratio) for rebalance date\n",
    "        value_factor_df = pd.DataFrame([value_df.loc[rebalance_date]], index=['btp']).T.dropna()\n",
    "\n",
    "        # Threshold for size\n",
    "        median = size_factor_df['mkt_cap'].median()\n",
    "\n",
    "        # Classify stocks into Low, Neutral, and High based on quantiles\n",
    "        lower = value_factor_df['btp'].quantile(0.3)\n",
    "        upper = value_factor_df['btp'].quantile(0.7)\n",
    "\n",
    "        # Merge the two\n",
    "        combined_df = size_factor_df.join(value_factor_df, how='inner')\n",
    "\n",
    "        # Classify for Size\n",
    "        combined_df['size_class'] = 'small'\n",
    "        combined_df.loc[combined_df['mkt_cap'] > median, 'size_class'] = 'big'\n",
    "\n",
    "        # Classify for Value\n",
    "        combined_df['value_class'] = 'neutral'\n",
    "        combined_df.loc[combined_df['btp'] <= lower, 'value_class'] = 'low'\n",
    "        combined_df.loc[combined_df['btp'] >= upper, 'value_class'] = 'high'\n",
    "        \n",
    "        # Create the FF Portfolios\n",
    "        combined_df['ff_class'] = combined_df['size_class'] + '_' + combined_df['value_class']\n",
    "        \n",
    "        # Market cap data\n",
    "        market_caps_df = pd.DataFrame([mktcap_df.loc[rebalance_date]], index=['mkt_cap']).T\n",
    "        \n",
    "        # Assign market caps to value classes\n",
    "        small_low_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'small_low'].index]\n",
    "        small_neutral_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'small_neutral'].index]\n",
    "        small_high_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'small_high'].index]\n",
    "        big_low_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'big_low'].index]\n",
    "        big_neutral_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'big_neutral'].index]\n",
    "        big_high_mktcap_df = market_caps_df.loc[combined_df[combined_df['ff_class'] == 'big_high'].index]\n",
    "        \n",
    "        # Compute weights\n",
    "        small_low_weights = small_low_mktcap_df['mkt_cap'] / small_low_mktcap_df['mkt_cap'].sum()\n",
    "        small_neutral_weights = small_neutral_mktcap_df['mkt_cap'] / small_neutral_mktcap_df['mkt_cap'].sum()\n",
    "        small_high_weights = small_high_mktcap_df['mkt_cap'] / small_high_mktcap_df['mkt_cap'].sum()\n",
    "        big_low_weights = big_low_mktcap_df['mkt_cap'] / big_low_mktcap_df['mkt_cap'].sum()\n",
    "        big_neutral_weights = big_neutral_mktcap_df['mkt_cap'] / big_neutral_mktcap_df['mkt_cap'].sum()\n",
    "        big_high_weights = big_high_mktcap_df['mkt_cap'] / big_high_mktcap_df['mkt_cap'].sum()\n",
    "        \n",
    "        # Store classifications and weights\n",
    "        quarterly_classifications[quarter] = {\n",
    "            \"small_low\": small_low_weights,\n",
    "            \"small_neutral\": small_neutral_weights,\n",
    "            \"small_high\": small_high_weights,\n",
    "            \"big_low\": big_low_weights, \n",
    "            \"big_neutral\": big_neutral_weights,\n",
    "            \"big_high\": big_high_weights,\n",
    "        }\n",
    "    \n",
    "    # Iterate over all available dates to compute daily returns\n",
    "    for date in common_index:\n",
    "        quarter_key = date.to_period('Q')  # Get quarter of the current date\n",
    "        \n",
    "        if quarter_key in quarterly_classifications:\n",
    "            # Retrieve stored classification and weights\n",
    "            small_low_weights = quarterly_classifications[quarter_key][\"small_low\"]\n",
    "            small_neutral_weights = quarterly_classifications[quarter_key][\"small_neutral\"]\n",
    "            small_high_weights = quarterly_classifications[quarter_key][\"small_high\"]\n",
    "            big_low_weights = quarterly_classifications[quarter_key][\"big_low\"]\n",
    "            big_neutral_weights = quarterly_classifications[quarter_key][\"big_neutral\"]\n",
    "            big_high_weights = quarterly_classifications[quarter_key][\"big_high\"]\n",
    "            \n",
    "            # Retrieve daily returns\n",
    "            target = pd.DataFrame([target_df.loc[date]], index=['returns']).T\n",
    "            \n",
    "            small_low_returns = target.reindex(small_low_weights.index).dropna()\n",
    "            small_neutral_returns = target.reindex(small_neutral_weights.index).dropna()\n",
    "            small_high_returns = target.reindex(small_high_weights.index).dropna()\n",
    "            big_low_returns = target.reindex(big_low_weights.index).dropna()\n",
    "            big_neutral_returns = target.reindex(big_neutral_weights.index).dropna()\n",
    "            big_high_returns = target.reindex(big_high_weights.index).dropna()\n",
    "            \n",
    "            # Compute portfolio returns\n",
    "            small_low_result = small_low_weights.reindex(small_low_returns.index).T @ small_low_returns\n",
    "            small_neutral_result = small_neutral_weights.reindex(small_neutral_returns.index).T @ small_neutral_returns\n",
    "            small_high_result = small_high_weights.reindex(small_high_returns.index).T @ small_high_returns\n",
    "            big_low_result = big_low_weights.reindex(big_low_returns.index).T @ big_low_returns\n",
    "            big_neutral_result = big_neutral_weights.reindex(big_neutral_returns.index).T @ big_neutral_returns\n",
    "            big_high_result = big_high_weights.reindex(big_high_returns.index).T @ big_high_returns\n",
    "            \n",
    "            # Store results\n",
    "            small_low_list.append(small_low_result.values[0] if not small_low_result.empty else None)\n",
    "            small_neutral_list.append(small_neutral_result.values[0] if not small_neutral_result.empty else None)\n",
    "            small_high_list.append(small_high_result.values[0] if not small_high_result.empty else None)\n",
    "            big_low_list.append(big_low_result.values[0] if not big_low_result.empty else None)\n",
    "            big_neutral_list.append(big_neutral_result.values[0] if not big_neutral_result.empty else None)\n",
    "            big_high_list.append(big_high_result.values[0] if not big_high_result.empty else None)\n",
    "\n",
    "    # Create final DataFrame\n",
    "    ff_portfolios = pd.DataFrame({\n",
    "        'small_high': small_high_list,\n",
    "        'small_neutral': small_neutral_list,\n",
    "        'small_low': small_low_list,\n",
    "        'big_high': big_high_list,\n",
    "        'big_neutral': big_neutral_list,\n",
    "        'big_low': big_low_list\n",
    "    }, index=common_index)\n",
    "    \n",
    "    return ff_portfolios"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb87c10c-9087-4311-96fd-7fd0da4a0855",
   "metadata": {},
   "source": [
    "# Create DataFrames\n",
    "\n",
    "ff_portfolio_returns = fama_and_french_decomposition(returns_df, mktcap_df, btp_df)\n",
    "\n",
    "ff_portfolio_returns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e73538eb-a548-4fc7-a51b-0155a42f271a",
   "metadata": {},
   "source": [
    "# Check the Annualized Mean Returns\n",
    "\n",
    "ff_portfolio_returns.mean().mul(100).mul(252)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a00368-559e-4108-8623-f227e0f74b58",
   "metadata": {},
   "source": [
    "# Check the Annualized Volatility\n",
    "\n",
    "ff_portfolio_returns.std().mul(100).mul(np.sqrt(252))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f72259e-fc63-49ed-8fb2-6a11f4a3ba1f",
   "metadata": {},
   "source": [
    "# Check the Risk Adjusted Returns\n",
    "\n",
    "ff_portfolio_returns.mean().mul(100).mul(252) / ff_portfolio_returns.std().mul(100).mul(np.sqrt(252))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75947df4-2bd2-4098-aae3-ad5f36ccc5a3",
   "metadata": {},
   "source": [
    "# Create Plot\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ff_portfolio_returns.cumsum(), label=ff_portfolio_returns.columns, alpha=1)\n",
    "\n",
    "# Config\n",
    "plt.title('Cumulative Returns Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeab3f16-379a-44f9-aed0-1568f75e39cc",
   "metadata": {},
   "source": [
    "# Create DataFrames\n",
    "\n",
    "ff_portfolio_betas = fama_and_french_decomposition(betas_df, mktcap_df, btp_df)\n",
    "\n",
    "ff_portfolio_betas"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39a74d5-2777-41d2-ade0-75b643a57b1f",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ff_portfolio_betas.ewm(span=21, adjust = False).mean(), label=ff_portfolio_betas.columns, alpha=1)\n",
    "plt.axhline(y=1, color='black', linestyle='dashed')\n",
    "\n",
    "# Config\n",
    "plt.title('Betas (Size Adjusted) Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Betas')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e8eaa1-138e-4494-b26a-080b675e62a3",
   "metadata": {},
   "source": [
    "# Calculate the SMB Premium\n",
    "\n",
    "SMB = (1/3)*(ff_portfolio_returns['small_low'] + ff_portfolio_returns['small_neutral'] + ff_portfolio_returns['small_high']) \\\n",
    "    - (1/3)*(ff_portfolio_returns['big_low'] + ff_portfolio_returns['big_neutral'] + ff_portfolio_returns['big_high'])\n",
    "\n",
    "SMB"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33610c0c-f549-4e32-b993-8f94a7886c69",
   "metadata": {},
   "source": [
    "# Plot SMB\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(SMB.cumsum(), label='SMB Premium', color = 'salmon', alpha=1)\n",
    "plt.axhline(y=0, color='black', linestyle='dashed')\n",
    "\n",
    "# Config\n",
    "plt.title('SMB Returns Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f31680-7541-4298-859c-04b45f68f380",
   "metadata": {},
   "source": [
    "# Calculate the HML Premium\n",
    "\n",
    "HML = (1/2)*(ff_portfolio_returns['small_high'] + ff_portfolio_returns['big_high']) \\\n",
    "    - (1/2)*(ff_portfolio_returns['small_low'] + ff_portfolio_returns['big_low'])\n",
    "\n",
    "HML"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "488df3a7-8416-4f9d-90f2-a94ef0c635fa",
   "metadata": {},
   "source": [
    "# Plot HML\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(HML.cumsum(), label='HML Premium', color = 'salmon', alpha=1)\n",
    "plt.axhline(y=0, color='black', linestyle='dashed')\n",
    "\n",
    "# Config\n",
    "plt.title('HML Returns Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d6b78e-0452-4797-95a6-72376a6942bc",
   "metadata": {},
   "source": [
    "# Create the market data\n",
    "daily_rfr = (((1 + (rfr['risk_free_rate'].div(100)))**(1/360)) - 1)\n",
    "benchmark_returns = sp500['sp_500'].pct_change(1)\n",
    "\n",
    "# Create the Excess Returns\n",
    "market_excess_returns = benchmark_returns - daily_rfr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "363ab27e-b847-4204-bda8-176dd74c5ab5",
   "metadata": {},
   "source": [
    "# Check the Correlation\n",
    "\n",
    "print(f'SMB premium correlation  with HML premium: {SMB.corr(HML)}')\n",
    "print(f'SMB premium correlation  with market premium: {SMB.corr(market_excess_returns)}')\n",
    "print(f'HML premium correlation  with market premium: {HML.corr(market_excess_returns)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e1925ed-42ae-4079-8c91-f530122e2aae",
   "metadata": {},
   "source": [
    "# Plot HML\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(market_excess_returns.cumsum(), label='Market Premium', alpha=1)\n",
    "plt.plot(SMB.cumsum(), label='SMB Premium', alpha=1)\n",
    "plt.plot(HML.cumsum(), label='HML Premium', alpha=1)\n",
    "plt.axhline(y=0, color='black', linestyle='dashed')\n",
    "\n",
    "# Config\n",
    "plt.title('HML Returns Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35a10c95-0752-4eee-9634-99e175b753a1",
   "metadata": {},
   "source": [
    "# Store both series\n",
    "\n",
    "SMB.to_csv(r\"..\\additional_data\\SMB.csv\")\n",
    "HML.to_csv(r\"..\\additional_data\\HML.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a91ad-18d7-4265-b84f-935799324f5c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
